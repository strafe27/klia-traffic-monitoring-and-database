{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f43b6d3-6d6f-4c30-a7d4-8bd191f57dd3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pytz\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e69a2f64-87a3-461f-824d-5ca1b5ad8c1e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n30\n"
     ]
    }
   ],
   "source": [
    "# Define the base path where the subfolders are located\n",
    "departures_base_path = '/mnt/raw/departures/'\n",
    "arrivals_base_path = '/mnt/raw/arrivals/'\n",
    "\n",
    "# Use a wildcard (*) to read all Parquet files in all subfolders\n",
    "df_departures = spark.read.parquet(f'{departures_base_path}*/')\n",
    "df_arrivals = spark.read.parquet(f'{arrivals_base_path}*/')\n",
    "\n",
    "\n",
    "departure_row = df_departures.count()\n",
    "arrivals_row = df_arrivals.count()\n",
    "\n",
    "print(departure_row)\n",
    "print(arrivals_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a3b7dfa-9d4f-4a35-b944-1431beec6f03",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_departures_sorted = df_departures.orderBy(\"actual_departure_myt\", ascending=True)\n",
    "df_arrivals_sorted = df_arrivals.orderBy(\"actual_arrival_myt\", ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f7b3318-41d9-4ac5-8ab9-b2988000b6cf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_arrivals_sorted = df_arrivals.orderBy(\"actual_arrival_myt\", ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bc65ed7-01af-4196-992e-39df859f98ce",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "folder_path_arrivals = f'/mnt/compiled/arrivals/'\n",
    "folder_path_departures = f'/mnt/compiled/departures/'\n",
    "\n",
    "# Coalesce the DataFrame to a single partition\n",
    "df_arrivals_coalesced = df_arrivals_sorted.coalesce(1)\n",
    "df_departures_coalesced = df_departures_sorted.coalesce(1)\n",
    "\n",
    "# Write the coalesced DataFrame to a Parquet file in the specified folder\n",
    "df_arrivals_coalesced.write.mode('overwrite').parquet(folder_path_arrivals)\n",
    "df_departures_coalesced.write.mode('overwrite').parquet(folder_path_departures)\n",
    "\n",
    "# List the files in the directory after writing the Parquet file\n",
    "files_arrivals = dbutils.fs.ls(folder_path_arrivals)\n",
    "files_departures = dbutils.fs.ls(folder_path_departures)\n",
    "\n",
    "# Correct the file path for renaming\n",
    "corrected_file_path_arrivals = f\"{folder_path_arrivals}arrivals_compiled.parquet\"\n",
    "corrected_file_path_departures = f\"{folder_path_departures}departures_compiled.parquet\"\n",
    "\n",
    "# Find the part file and rename it (for arrivals)\n",
    "for file in files_arrivals:\n",
    "    if file.name.startswith(\"part-\"):\n",
    "        # Move (rename) the part file to the desired file name\n",
    "        dbutils.fs.mv(file.path, corrected_file_path_arrivals)\n",
    "    elif file.name.startswith(\"_SUCCESS\") or file.name.startswith(\"_committed\") or file.name.startswith(\"_started\"):\n",
    "        # Remove unwanted system files like _SUCCESS, _committed, _started\n",
    "        dbutils.fs.rm(file.path)\n",
    "\n",
    "# Find the part file and rename it (for departures)\n",
    "for file in files_departures:\n",
    "    if file.name.startswith(\"part-\"):\n",
    "        # Move (rename) the part file to the desired file name\n",
    "        dbutils.fs.mv(file.path, corrected_file_path_departures)\n",
    "    elif file.name.startswith(\"_SUCCESS\") or file.name.startswith(\"_committed\") or file.name.startswith(\"_started\"):\n",
    "        # Remove unwanted system files like _SUCCESS, _committed, _started\n",
    "        dbutils.fs.rm(file.path)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "hourly to compiled",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
